{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyDSout: DataStream requests made simple\n",
    "\n",
    "**Author:** Marcus Bravidor ([marcus.bravidor@hhu.de](mailto:marcus.bravidor@hhu.de))  \n",
    "**Version:** PyDSout 0.1  \n",
    "**Date:** 2018-04-04  \n",
    "**License:** [MIT License](https://opensource.org/licenses/MIT) \n",
    "\n",
    "**Purpose:** Automatically request firm-level financial data (e.g., WorldScope) from Thomson Reuters DataStream and save the results to a long format CSV file (separator: semicolon or \";\").\n",
    "\n",
    "**A note on the example below:** Request for 20 items on 5 German companies.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Loading PyDataStream and Login. Output of all accessable databases. _Datastream_ should be one of them.  \n",
    "Replace `username`and `password` with the credentials supplied by your institution or company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TMT',\n",
       " 'Resolution',\n",
       " 'Symbology',\n",
       " 'XREF',\n",
       " 'Economics',\n",
       " 'ILXAUTH',\n",
       " 'Datastream',\n",
       " 'ILXMLP']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydatastream import Datastream\n",
    "from suds.client import Client\n",
    "DWE = Datastream(username=\"USERNAME\", password=\"PASSWORD\")\n",
    "DWE.system_info()\n",
    "DWE.sources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requesting time series data\n",
    "\n",
    "### General\n",
    "\n",
    "The request has a standardized structure: _Ticker_*~*_Item1, Item2, ItemN_*~*_EndDate_*~*_StartDate_*~*_Frequency_\n",
    "\n",
    "Legend:\n",
    "* *Ticker:* Ticker symbol from DataStream including possible special characters (@, :, etc.)\n",
    "* *Item 1-Item N:* Item mnemonic from DataStream (e.g., WC...)\n",
    "* *EndDate:* Date for last request in ISO format (YYYY-MM-DD)\n",
    "* *StartDate:* Date for first request in ISO format (YYYY-MM-DD)\n",
    "* *Frequency:* Number of points in time. Possible: Yearly (Y), Monthly (M), Weekly (W), Daily (D)\n",
    "\n",
    "### Customization\n",
    "\n",
    "Use the following steps to start a customized request:\n",
    "\n",
    "* Change `startdate`, `enddate`, and `freq` to the desired values.\n",
    "* Firms to query: Add a file named `firms.txt` to the `root` directory. Format:\n",
    "\n",
    "```\n",
    "DSID1\n",
    "DSID2\n",
    "DSID3\n",
    "\n",
    "756944\n",
    "866922\n",
    "278419\n",
    "```\n",
    "\n",
    "* Add a file name `items.txt` to the `root` directory. Format:\n",
    "\n",
    "```\n",
    "code1,code2,code3,\n",
    "NOSHFF,WC05301,WC07101,\n",
    "```\n",
    "\n",
    "### Limitations\n",
    "\n",
    "**15 item limit**  \n",
    "One request contains up to 15 items for all points in time (e.g., years) per firm. If you supply more than 15 items, the script automatically generates multiple requests per firm and merges the data in a later stage.  \n",
    "Why the 15 items limit? From my tests, DataStream does not deliver more than that in one request. But feel free to try other values by changing `15` in line `itemslices = list(iterutils.chunked_iter(items[0], 15))`.  \n",
    "\n",
    "**Multiple Requests at once**  \n",
    "PyDataStream offers the option to [perform multiple requests at once](https://github.com/vfilimonov/pydatastream#debugging-and-error-handling) which would most likely make the script much faster. However, I did not implement this step, yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging information\n",
      "\n",
      "firm 1 of 5 started.\n",
      "firm 1 of 5 finished.\n",
      "firm 2 of 5 started.\n",
      "firm 2 of 5 finished.\n",
      "firm 3 of 5 started.\n",
      "firm 3 of 5 finished.\n",
      "firm 4 of 5 started.\n",
      "firm 4 of 5 finished.\n",
      "firm 5 of 5 started.\n",
      "firm 5 of 5 finished.\n",
      "All requests completed.\n"
     ]
    }
   ],
   "source": [
    "startdate = '2005-12-31'\n",
    "enddate = '2016-12-31'\n",
    "freq = 'Y'\n",
    "\n",
    "import codecs, csv\n",
    "with codecs.open(\"firms.txt\", \"rb\", \"utf-8\") as file:\n",
    "    f = csv.reader(file, delimiter=\"\\n\")\n",
    "    firms = list(f)\n",
    "    numfirms = len(firms)\n",
    "\n",
    "with codecs.open(\"items.txt\", \"rb\", \"utf-8\") as file:\n",
    "    items = csv.reader(file, delimiter=\",\")\n",
    "    items = list(items)\n",
    "\n",
    "    i = 0\n",
    "    nr = list()\n",
    "    d = {}\n",
    "    from boltons import iterutils\n",
    "    itemslices = list(iterutils.chunked_iter(items[0], 15))\n",
    "    numslices = len(itemslices)\n",
    "    print (\"Debugging information\")\n",
    "    print (\"\")\n",
    "    for x in firms:\n",
    "        j = 0\n",
    "        d[\"raw{0}\".format(i)] = {}\n",
    "        print (\"firm \"+str(i+1)+\" of \"+str(numfirms)+\" started.\")\n",
    "        for y in itemslices:\n",
    "            types = ','.join(y)\n",
    "            d[\"raw{0}\".format(i)][\"raw{0}\".format(j)] = DWE.request(str(x[0])+\"~=\"+str(types)+\"~\"+str(enddate)+\"~\"+str(startdate)+\"~\"+str(freq))\n",
    "            j = j+1\n",
    "        print (\"firm \"+str(i+1)+\" of \"+str(numfirms)+\" finished.\")\n",
    "        i = i+1\n",
    "print (\"All requests completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the unstructured query response to a structured list\n",
    "\n",
    "**First Step: Convert the unstructured `suds` response to `JSON` format.** Code is unchanged from the suggestion by _radtek_ on [StackOverflow](https://stackoverflow.com/questions/17581731/parsing-suds-soap-complex-data-type-into-python-dict/17582946).\n",
    "\n",
    "**Second Step: Convert the `JSON` data to a three-dimensional list.** The resulting list is structured as follows:\n",
    "\n",
    "* First level: One entry per firm.\n",
    "* Second level (rows in table below): One entry per item.\n",
    "* Third level (columns in table below): One entry for (each item) per year. The only notable exemptions are static items (e.g., `DISPNAME`) with only one corresponding value (column 0).\n",
    "\n",
    "Per firm, the strucutre looks as follows:\n",
    "\n",
    "| item/value | 0 | 1 | 2 | ... | n |\n",
    "|:---:|---:|---:|---:|:---:|---:|\n",
    "| 0 | CCY | '' | | ... | |\n",
    "| 1 | DATE | 2010-12-31T00:00:00 | 2011-12-31T00:00:00 | ... | 2016-12-31T00:00:00 |\n",
    "| 2 | DISPNAME | CJ VOGEL FUR BETEILIG. DEAD - 26/03/14 | | ... | |\n",
    "| 3 | WC05301 | 693.0 | 693.0 | ... | nan |\n",
    "| ... | ... | ... | ...| ... | ... |\n",
    "| n | ... | ... | ...| ... | ... |\n",
    "\n",
    "The list containing all firms and datapoints is called `dobject`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started converting data of firm 0\n",
      "Finished converting data of firm 0\n",
      "Started converting data of firm 1\n",
      "Finished converting data of firm 1\n",
      "Started converting data of firm 2\n",
      "Finished converting data of firm 2\n",
      "Started converting data of firm 3\n",
      "Finished converting data of firm 3\n",
      "Started converting data of firm 4\n",
      "Finished converting data of firm 4\n",
      "Conversion completed.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def basic_sobject_to_dict(obj):\n",
    "    \"\"\"Converts suds object to dict very quickly.\n",
    "    Does not serialize date time or normalize key case.\n",
    "    :param obj: suds object\n",
    "    :return: dict object\n",
    "    \"\"\"\n",
    "    if not hasattr(obj, '__keylist__'):\n",
    "        return obj\n",
    "    data = {}\n",
    "    fields = obj.__keylist__\n",
    "    for field in fields:\n",
    "        val = getattr(obj, field)\n",
    "        if isinstance(val, list):\n",
    "            data[field] = []\n",
    "            for item in val:\n",
    "                data[field].append(basic_sobject_to_dict(item))\n",
    "        else:\n",
    "            data[field] = basic_sobject_to_dict(val)\n",
    "    return data\n",
    "\n",
    "\n",
    "def sobject_to_dict(obj, key_to_lower=False, json_serialize=False):\n",
    "    \"\"\"\n",
    "    Converts a suds object to a dict.\n",
    "    :param json_serialize: If set, changes date and time types to iso string.\n",
    "    :param key_to_lower: If set, changes index key name to lower case.\n",
    "    :param obj: suds object\n",
    "    :return: dict object\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "\n",
    "    if not hasattr(obj, '__keylist__'):\n",
    "        if json_serialize and isinstance(obj, (datetime.datetime, datetime.time, datetime.date)):\n",
    "            return obj.isoformat()\n",
    "        else:\n",
    "            return obj\n",
    "    data = {}\n",
    "    fields = obj.__keylist__\n",
    "    for field in fields:\n",
    "        val = getattr(obj, field)\n",
    "        if key_to_lower:\n",
    "            field = field.lower()\n",
    "        if isinstance(val, list):\n",
    "            data[field] = []\n",
    "            for item in val:\n",
    "                data[field].append(sobject_to_dict(item, json_serialize=json_serialize))\n",
    "        else:\n",
    "            data[field] = sobject_to_dict(val, json_serialize=json_serialize)\n",
    "    return data\n",
    "\n",
    "\n",
    "def sobject_to_json(obj, key_to_lower=False):\n",
    "    \"\"\"\n",
    "    Converts a suds object to json.\n",
    "    :param obj: suds object\n",
    "    :param key_to_lower: If set, changes index key name to lower case.\n",
    "    :return: json object\n",
    "    \"\"\"\n",
    "    data = sobject_to_dict(obj, key_to_lower=key_to_lower, json_serialize=True)\n",
    "    return json.dumps(data)\n",
    "\n",
    "firmindex = 0\n",
    "dobject = []\n",
    "for response in d.keys():\n",
    "    firmlist = []\n",
    "    print (\"Started converting data of firm \"+str(firmindex))\n",
    "    for r in d[response].keys():\n",
    "        test = sobject_to_json(d[response][r])\n",
    "        jsonobject = json.loads(test)\n",
    "        # Debugging: Show Instrument (= DataStream query)\n",
    "        # print (jsonobject['Instrument'])\n",
    "        # Debugging: Print complete JSON object\n",
    "        # print (jsonobject)\n",
    "        try:\n",
    "            for i in jsonobject['Fields']['Field']:\n",
    "                itemlist = []\n",
    "                # Debugging: Show single items included in response\n",
    "                # print (\"Loop over Field-List-Items that are dictionaries\")\n",
    "                # Debugging: Show item name\n",
    "                # print (i['Name'])\n",
    "                itemlist.append(i['Name'])\n",
    "                try:\n",
    "                    # Debugging: Check if content is Array and show contents\n",
    "                    # print (\"Is Dictionary.\")\n",
    "                    # print (i['ArrayValue'])\n",
    "                    for value in i['ArrayValue']['anyType']:\n",
    "                        # Debugging: Show values in array\n",
    "                        # print (value)\n",
    "                        itemlist.append(value)\n",
    "                except KeyError:\n",
    "                    # Debugging: Content must be single value, show value\n",
    "                    # print (\"Is Value.\")\n",
    "                    # print (i['Value'])\n",
    "                    itemlist.append(i['Value'])\n",
    "                firmlist.append(itemlist)\n",
    "        except TypeError:\n",
    "            pass\n",
    "    dobject.append(firmlist)\n",
    "    print (\"Finished converting data of firm \"+str(firmindex))\n",
    "    firmindex = firmindex + 1\n",
    "print (\"Conversion completed.\")\n",
    "# Debugging: Show complete converted data object\n",
    "# print (\"Complete Data Object:\")\n",
    "# print (dobject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to CSV\n",
    "\n",
    "For each firm, `dobject` is converted to a `pandas` dataframe. Data is reshaped to conform to the *long format*. All originating dataframes are merged.\n",
    "\n",
    "In most cases, not all items are available. To deal with the issue of missing data values (and the resulting `key indexing error` for the lists), numerous `try` and `except` specification are included.\n",
    "\n",
    "Results are written to the semicolon-separated CSV file called `ds_output.csv`. \n",
    "\n",
    "Excerpt from the resulting file (first two items):\n",
    "\n",
    "```\n",
    ";ccy;date;dispname;frequency;insterror;symbol;wc01149;wc01150;\n",
    "0;;2005-12-31T00:00:00;CENTROTEC SUSTAIN;Y;\"FAILED: $$\"\"ER\"\", 4540, NO DATA VALUES FOUND\";688137;932.0;;17763.0;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1 of 4 completed.\n",
      "Merge 2 of 4 completed.\n",
      "Merge 3 of 4 completed.\n",
      "Merge 4 of 4 completed.\n",
      "CSV Export completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_records(dobject[0])\n",
    "df = df.transpose()\n",
    "varnames = df.iloc[0]\n",
    "j = 0\n",
    "for i in range(0, len(varnames)):\n",
    "    try:\n",
    "        # Debugging: Show all variables in first response data set\n",
    "        # print (varnames[i])\n",
    "        df.rename(columns={j: varnames[i].lower()}, inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    j += 1\n",
    "df.drop(0, inplace=True)\n",
    "df = df.loc[:,~df.columns.duplicated()]\n",
    "try:\n",
    "    df['dispname'].fillna(method='ffill', inplace=True)\n",
    "except KeyError:\n",
    "    pass\n",
    "try:\n",
    "    df['frequency'].fillna(method='ffill', inplace=True)\n",
    "except KeyError:\n",
    "    pass\n",
    "try:\n",
    "    df['insterror'].fillna(method='ffill', inplace=True)\n",
    "except KeyError:\n",
    "    pass\n",
    "try:\n",
    "    df['symbol'].fillna(method='ffill', inplace=True)\n",
    "except KeyError:\n",
    "    pass\n",
    "# Debugging: Show first data set\n",
    "# print(\"Start Data set:\")\n",
    "# print(df)\n",
    "\n",
    "k = 1\n",
    "for i in range(1,len(dobject)):\n",
    "    df2 = pd.DataFrame.from_records(dobject[i])\n",
    "    df2 = df2.transpose()\n",
    "    varnames = df2.iloc[0]\n",
    "    j = 0\n",
    "    for i in range(0, len(varnames)):\n",
    "        try:\n",
    "            # Debugging: Show variables in using data set\n",
    "            # print (varnames[i])\n",
    "            df2.rename(columns={j: varnames[i].lower()}, inplace=True)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        j += 1\n",
    "    df2.drop(0, inplace=True)\n",
    "    df2 = df2.loc[:,~df2.columns.duplicated()]\n",
    "    try:\n",
    "        df2['dispname'].fillna(method='ffill', inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        df2['frequency'].fillna(method='ffill', inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        df2['insterror'].fillna(method='ffill', inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        df2['symbol'].fillna(method='ffill', inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    # Debugging: Show using data set\n",
    "    # print(\"Using data set:\")\n",
    "    # print(df2)\n",
    "    df = pd.concat([df, df2], ignore_index=True)\n",
    "    print (\"Merge \"+str(k)+\" of \"+str(len(dobject)-1)+\" completed.\")\n",
    "    # Debugging: Show merged data set\n",
    "    # print(\"Merged data set:\")\n",
    "    # print(df)\n",
    "    k += 1\n",
    "\n",
    "# write result to csv file\n",
    "df.to_csv('ds_output.csv', sep=';', encoding='utf-8')\n",
    "print (\"CSV Export completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
